{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classification with PyTorch\n",
    "\n",
    "Classification is a problem of predicting whether something is one thing or another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Make classification data and get it ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Create circles\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 smaples of X:\\n {X[:5]}\")\n",
    "print(f\"First 5 smaples of y:\\n {y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DataFrame of circle data\n",
    "import pandas as pd\n",
    "circles = pd.DataFrame({\"X1\": X[:,0],\n",
    "                        \"X2\": X[:,1],\n",
    "                        \"label\": y\n",
    "                        })\n",
    "\n",
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=X[:,0], y=X[:,1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The data we are working with is often referred to as toy dataset, a dataset that is small enough to experiment but still sizeable enough to practice the fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Check Input and output shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first example of features and labels\n",
    "X_sample =X[0]\n",
    "y_sample = y[0]\n",
    "\n",
    "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
    "print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Turn data into tensors and create train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "X[:5], y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X), X.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 0.2 = 20% of data will be test & 80% will be train\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train),len(X_test),len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model \n",
    "Let's build a model to classify our blue and red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available () else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have setup device agnostic code, let's create a model that:\n",
    "\n",
    "1. Subclasses `nn.Module` (almost all models in pytorch subclass `nn.Module`)\n",
    "2. Create 2 `nn.Linear()` layers that are capable of handling the shapes of our data\n",
    "3. Define a `forward()` method that outlines the forward pass (or forward computation) of the model\n",
    "4. Instantiate an instance of our model class and send it to the target `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Construct a model that subclassess nn.Module\n",
    "class  CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2. Create 2 nn.Linear layera capable of handling the shapes of our data\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features from previous layer and ouptputs a single layer of 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x)) # x -> layer_1 -> layer_2 -> output\n",
    "\n",
    "\n",
    "# 4. Instantiate an instance of our model class and send it to the target device\n",
    "\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(model_0.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Replicate the model above using nn.Sequential()\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "with torch.inference_mode():\n",
    "    untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of prediction: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test smaples: {len(X_test)}, Shape: {X_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions:\\n {torch.round(untrained_preds[:10])}\")\n",
    "print(f\"\\nFirst 10 labels:\\n {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss function\n",
    "# loss_fn = nn.BCELoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss() # it have sigmoid activation function built-in\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy - out of 100 examples, what percentage does our model get right?\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct/len(y_pred)) *100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train model\n",
    "To train our model, we need to create a training loop with the following stpes\n",
    "\n",
    "1. Forward pass\n",
    "2. Calculate the loss\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward (backpropagation)\n",
    "5. Optimizer step(gradient descnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Going from raw logits -> Prediction probabilites -> prediction labels\n",
    "\n",
    "Our model outputs are going to be raw **logits**\n",
    "\n",
    "We can convert these logits into prediction probabilities by passing them to some kind of activation function (e.g. sigmoid for binary classification and softmax for multiclass classification).\n",
    "\n",
    "Then we can convert our model's prediction probabilities to prediction labels by either rounding them or taking the argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 outputs of the forward pass on the test data\n",
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sigmoid activation function on our model logits\n",
    "y_preds_probs = torch.sigmoid(y_logits)\n",
    "y_preds_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our prediction probability values, we need to perform range style rounding on them:\n",
    "\n",
    "y_preds_probs >= 0.5, y=1 (class 1)\n",
    "y_preds_probs < 0.5, y=0 (class 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the predicted labels\n",
    "y_preds = torch.round(y_preds_probs)\n",
    "\n",
    "# In full (logits -> preds probs -> pred labels)\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "\n",
    "# Check for equality\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# Get rid of extra dimension\n",
    "y_preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building a training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    #Training\n",
    "    model_0.train().to(device)\n",
    "\n",
    "    # 1. Forward Pass\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. Calculate the loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), y_train)\n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
    "                    y_train)\n",
    "\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred= y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer (perform gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # 1. Do forward pass\n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc: .2f}%\")\n",
    "        # Print out model state dict\n",
    "        # print(model_0.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Predictions and evaluate the model\n",
    "\n",
    "From the metrics it looks like our model isn't learning anything...\n",
    "\n",
    "So to inspect it let's make some predictions and make them visual\n",
    "\n",
    "https://madewithml.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Download helper funtions from Learn PyTorch repo (if it's not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "    print(\"Download helper_functions.py\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        f.write(request.content)\n",
    "from helper_functions import plot_predictions, plot_decision_boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot decision boundary of the model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_0, X_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_0, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improving a model (from a model perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CircleModelV1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n",
    "        self.layer_2 = nn.Linear(in_features=10, out_features= 10)\n",
    "        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # z = self.layer_1(x)\n",
    "        # z = self.layer_2(z)\n",
    "        # z = self.layer_3(z)\n",
    "        return self.layer_3(self.layer_2(self.layer_1(x)))\n",
    "\n",
    "model_1 = CircleModelV1().to(device)\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)\n",
    "\n",
    "# Write a training and evaluation loop for model_1\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    #Training\n",
    "    model_1.train()\n",
    "\n",
    "    # 1. Forward Pass\n",
    "    y_logits = model_1(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "\n",
    "    # 2. Calculate the loss/accuracy\n",
    "    # loss = loss_fn(torch.sigmoid(y_logits), y_train)\n",
    "    loss = loss_fn(y_logits, # nn.BCEWithLogitsLoss expects raw logits as input\n",
    "                    y_train)\n",
    "\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred= y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Perform backpropagation on the loss with respect to the parameters of the model\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer (perform gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_1.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # 1. Do forward pass\n",
    "        test_logits = model_1(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate the loss\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out what's happening\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc: .2f}%\")\n",
    "        # Print out model state dict\n",
    "        # print(model_0.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decision boundary of the model\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_1, X_train, y_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preparing data to see if our model can fit straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "start = 0\n",
    "end = 1\n",
    "step =0.01\n",
    "\n",
    "# Create data\n",
    "X_regression = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y_regression = weight * X_regression + bias # Linear regression formula\n",
    "\n",
    "# Check the data\n",
    "print(len(X_regression))\n",
    "X_regression[:5], y_regression[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test splits\n",
    "train_split = int(0.8* len(X_regression))\n",
    "X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\n",
    "X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n",
    "\n",
    "len(X_train_regression), len(X_test_regression), len(y_train_regression), len(y_test_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(train_data= X_train_regression, train_labels=y_train_regression, test_data=X_test_regression, test_labels=y_test_regression);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ADjusting model_1 to fit straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture as model_1\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Linear(in_features=1, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put the data on the target device\n",
    "X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\n",
    "X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model_2(X_train_regression)\n",
    "    loss = loss_fn(y_pred, y_train_regression)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_2.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_2(X_test_regression)\n",
    "        test_loss= loss_fn(test_pred, y_test_regression)\n",
    "\n",
    "    # Print\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss:.5f} | Test loss: {test_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn model into evaluation mode\n",
    "model_2.eval()\n",
    "\n",
    "# Make predictions on the test data\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_2(X_test_regression)\n",
    "\n",
    "\n",
    "\n",
    "plot_predictions(train_data= X_train_regression.cpu(), train_labels=y_train_regression.cpu(), test_data=X_test_regression.cpu(), test_labels=y_test_regression.cpu(), predictions=y_preds.cpu());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The missing piece: non-linearity\n",
    "\"What patterns could you draw if you were given an infinite amount of straight and non-straight lines?\"\n",
    "\n",
    "Or in machine learning terms, an infinite amount of linear and non-linear functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Recreating non-linear data (red and blue circles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and plot a data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "X, y = make_circles(n_samples, noise=0.03, random_state=42)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to tensors then to train and test splits\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Turn data into tensors\n",
    "X = torch.from_numpy(X).type(torch.float)\n",
    "y = torch.from_numpy(y).type(torch.float)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42);\n",
    "\n",
    "X_train[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Building a model with non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with non-linear activation functions\n",
    "from torch import  nn\n",
    "class CircleModelV2(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=30)\n",
    "        self.layer_2 = nn.Linear(in_features=30, out_features=30)\n",
    "        self.layer_3 = nn.Linear(in_features=30, out_features=30)\n",
    "        self.layer_4 = nn.Linear(in_features=30, out_features=30)\n",
    "        self.layer_5 = nn.Linear(in_features=30, out_features=1)\n",
    "        self.relu = nn.ReLU() # relu is a non-linear activation function\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Where should we put our non-linear activation functions?\n",
    "        return self.layer_5(self.relu(self.layer_4(self.relu(self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))))))\n",
    "\n",
    "model_3 = CircleModelV2().to(device)\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss() # it have sigmoid activation function built-in\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_3.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Building training and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model_3.train().to(device)\n",
    "\n",
    "    # 1. Forward Pass\n",
    "    y_logits = model_3(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "\n",
    "    #2. Calculate the loss\n",
    "    loss = loss_fn(y_logits, y_train)\n",
    "\n",
    "    acc = accuracy_fn(y_true=y_train, y_pred=y_pred)\n",
    "\n",
    "    # 3. Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Backproagation\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Step the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Testing\n",
    "    model_3.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model_3(X_test).squeeze()\n",
    "        test_pred =torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_test)\n",
    "\n",
    "        test_acc = accuracy_fn(y_true=y_test, y_pred=test_pred)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc: .2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Evauating a model trained with non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "model_3.eval()\n",
    "with torch.inference_mode():\n",
    "    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\n",
    "y_preds[:10], y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decisition boundaries\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_1, X_train, y_train) # model_1 no non-linearity\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_3, X_test, y_test) # model_3 no non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Replicating non-linear activation functions\n",
    "Neural networks, rather than us telling the model what to learn, we give it the tools to discover patterns in data and it tries to figure out the patterns on its own.\n",
    "\n",
    "And these tools are linear and non-linear functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor\n",
    "A = torch.arange(-10, 10, 1,  dtype=torch.float32)\n",
    "A.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tenor\n",
    "plt.plot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.relu(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x:torch.Tensor) -> torch.tensor:\n",
    "    return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n",
    "\n",
    "relu(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ReLU activation function\n",
    "plt.plot(relu(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 /(1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.sigmoid(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigmoid(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Putting it all together with a multi class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8.1 Creating a toy multi-class dataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the hyperparameters for data creation\n",
    "NUM_CLASSES = 4\n",
    "NUM_FEATURES = 2\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# 1. Create multi class data\n",
    "X_blob, y_blob = make_blobs(n_samples=1000, n_features=NUM_FEATURES, centers=NUM_CLASSES, cluster_std= 1.5, random_state=RANDOM_SEED)\n",
    "\n",
    "# 2. Turn data into tensors\n",
    "X_blob = torch.from_numpy(X_blob).type(torch.float)\n",
    "y_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\n",
    "\n",
    "# 3. Split into train and test\n",
    "X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob, y_blob, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# Plot data\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.scatter(X_blob[:,0], X_blob[:,1],c=y_blob, cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Building a multi class classification model in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a multi class classification model\n",
    "class BlobModel(nn.Module):\n",
    "    def __init__(self, input_features, output_features, hidden_units=8):\n",
    "        \"\"\"Initializes multi-class classification model\n",
    "\n",
    "        Args:\n",
    "            input_features(int): Number of input features to the model\n",
    "            output_features(int): Number of outputs features(number of output classes)\n",
    "            hidden_units(int): Number of hidden units between layers, default 8\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=input_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_features)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear_layer_stack(x)\n",
    "\n",
    "# Create an instace of BlobModel and send it to target device\n",
    "model_4 = BlobModel(input_features=2, output_features=4,  hidden_units=8).to(device)\n",
    "\n",
    "model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_blob_train.shape, y_blob_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Create a loss function and an optimizer for a multi class classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function for multi-class classification\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create an optimizer for multi class classification\n",
    "optimizer = torch.optim.SGD(params=model_4.parameters(), lr=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Getting prediction probabilities for a multi class PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some raw outputs of our model(logits)\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_4(X_blob_test.to(device))\n",
    "\n",
    "y_logits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_blob_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our model logit outpouts to predictions probabilities\n",
    "y_pred_probs = torch.softmax(y_logits, dim=1)\n",
    "print(y_logits[:5])\n",
    "print(y_pred_probs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our model prediction probabilities to prediction labels\n",
    "y_preds = torch.argmax(y_pred_probs, dim=1)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_blob_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Creating a training loop and testing loop for a multi class PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the multi class model to the data\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Put data to the target device\n",
    "X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\n",
    "X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n",
    "\n",
    "# Loop through data\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_4.train()\n",
    "\n",
    "    y_logits = model_4(X_blob_train)\n",
    "    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "    loss = loss_fn(y_logits,y_blob_train)\n",
    "    acc = accuracy_fn(y_true=y_blob_train, y_pred=y_pred)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    ### Testing\n",
    "    model_4.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_logits = model_4(X_blob_test)\n",
    "        test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "\n",
    "        test_loss = loss_fn(test_logits, y_blob_test)\n",
    "        test_acc = accuracy_fn(y_true=y_blob_test, y_pred=test_pred)\n",
    "\n",
    "    # Print out\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test acc: {test_acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Making and evaluation prediction with a PyTorch multi class model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predicitons\n",
    "model_4.eval()\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_4(X_blob_test)\n",
    "\n",
    "# View the first 10 predictions\n",
    "y_logits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go from logits -> prediciton probabilites\n",
    "y_preds_probs = torch.softmax(y_logits, dim=1)\n",
    "y_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go from pred prbs to pred labels\n",
    "y_preds = torch.argmax(y_preds_probs, dim=1)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Train\")\n",
    "plot_decision_boundary(model_4, X_blob_train,y_blob_train)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"Test\")\n",
    "plot_decision_boundary(model_4, X_blob_test, y_blob_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. A few model classification metrics (to evaluate our classification model)\n",
    "\n",
    "* Accuracy - out of 100 samples, how many does our model get right?\n",
    "* Precision \n",
    "* Recall\n",
    "* F1-score\n",
    "* Confusion matrix\n",
    "* Classification report\n",
    "\n",
    "See this article for when to use precision/recall: https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "\n",
    "if you want access to a lot of PyTorch metrics, see TrochMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "# Setup metric\n",
    "torchmetric_accuracy = Accuracy(task='multiclass', num_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "# Calculate accuracy\n",
    "torchmetric_accuracy(y_preds, y_blob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
